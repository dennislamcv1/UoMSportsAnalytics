{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-e2ac1ea7-7831-4cc3-a8de-446a21ab685a",
    "deepnote_cell_type": "markdown",
    "etc_hash": "41b20507751f3db2caa5f6d5d153fb656cfe9b298521ad2d49a23149c32d1c59",
    "tags": []
   },
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data we're going to go back to the MLB and our pitch prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-f4a438d1-fce5-4351-8eaf-8c52f9b4e359",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "52b29eaa97d30f5a6f36c9eeef5b5b528fdcb52f6bc2a2c437eb6626be14a9e3",
    "execution_millis": 1480,
    "execution_start": 1621778384611,
    "source_hash": "82d04f3b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import zipfile\n",
    "\n",
    "# Read in our data\n",
    "filename=\"assets/baseball_svm_data.zip\"\n",
    "df=pd.read_csv(zipfile.ZipFile(filename).open(\"reg_Sep2019.csv\"))\n",
    "\n",
    "# Setup the features\n",
    "pitch_metrics=['release_spin_rate','release_extension','release_pos_y','release_pos_x','release_pos_z','effective_speed']\n",
    "player_metrics=['player_name']\n",
    "game_details=['outs_when_up','inning']\n",
    "features=[*pitch_metrics, *player_metrics, *game_details]\n",
    "\n",
    "# Clean as before\n",
    "df=df.dropna(subset=[\"pitch_type\"])\n",
    "df=df[df[\"pitch_type\"]!=\"EP\"]\n",
    "df['player_name']=df['player_name'].factorize()[0]\n",
    "df['pitch_numeric']=df['pitch_type'].factorize()[0]\n",
    "df=df[[*features, \"pitch_numeric\", \"pitch_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's create our training and validation sets. This time I'm going \n",
    "# to stratify our sample across the different kinds of pitches which\n",
    "# exist.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[features],\n",
    "                                                  df[\"pitch_numeric\"],\n",
    "                                                  train_size=0.1,\n",
    "                                                  random_state=1337, \n",
    "                                                  stratify=df[\"pitch_numeric\"])\n",
    "\n",
    "# Then we can fill missing values with the mean. Remember two things when doing this:\n",
    "# (a) choosing the mean is an arbitrary choice on my part! It might not make sense always!\n",
    "# (b) you *must* do this *after* you split your training/validation sets, these two\n",
    "# datasets must be treated as independent\n",
    "X_train=X_train.fillna(X_train.mean())\n",
    "X_val=X_val.fillna(X_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can verify that these are correctly stratified by looking at at the histograms\n",
    "# of each, they should look the same as proportions\n",
    "fig, axes = plt.subplots(2, figsize=(8,8))\n",
    "axes[0].hist(y_train, density=True)\n",
    "axes[1].hist(y_val, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier\n",
    "So let's look at a voting classifier first. This is just a majority vote of the models which we have trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we import our ensemble classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# The nice thing with ensembles is the very first argument is some list of\n",
    "# estimators (classifiers) - so all we really have to do is put together a\n",
    "# dictonary of classifiers we want to use!\n",
    "clfs={}\n",
    "\n",
    "# Let's create a couple of different of descision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# - one which has only a depth of 3\n",
    "clfs[\"dt0\"]=DecisionTreeClassifier(max_depth=3, random_state=1337)\n",
    "# - one which requires there to be at least 7 samples at each leaf\n",
    "clfs[\"dt1\"]=DecisionTreeClassifier(min_samples_leaf=7, random_state=1337)\n",
    "# - and one which balances the classes since we have unbalanced data\n",
    "clfs[\"dt2\"]=DecisionTreeClassifier(class_weight=\"balanced\", random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles are just another classifier, but many of the ensembles, like\n",
    "# voting ensembles, can do work in parallel. Here I'm setting the n_jobs\n",
    "# parameter to -1, which tells the classifier to use all system CPUs\n",
    "voters=VotingClassifier(estimators=clfs.items(), n_jobs=-1)\n",
    "\n",
    "# And then we just fit the data\n",
    "voters.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now the voters variable is this collection of three different \n",
    "# models which have been trained on this dataset, and we can do things \n",
    "# like look at the accuracy or other evaluation measures\n",
    "voters.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, we're interested in performance in our validation dataset\n",
    "# so we can look at that too\n",
    "voters.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that our training data accuracy score is much higher than our validation accuracy score. This is to be expected -- all of the previous comments about using cross fold validation to improve your understanding of the accuracy, or not using accuracy at all and instead looking at something like a confusion matrix, those all still apply. But instead of tuning this more, I want to move on to our next ensemble technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier\n",
    "Recall that the bagging approach to ensembles creates a number of different classifiers but does so from a single model and acts more like cross validation, pulling out random subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# In this approach we only have one model definition so let's use one of \n",
    "# our descision trees. We can set how many classifiers we want the bagger\n",
    "# to use, as well using the n_estimators parameter. There are many other\n",
    "# parameters to use, I'll use one here which sets the maximum number of\n",
    "# features each classifier in the bag can use at 70%\n",
    "bagger=BaggingClassifier(base_estimator=clfs[\"dt0\"],\n",
    "                        n_estimators=10,\n",
    "                        max_features=0.7,\n",
    "                        n_jobs=-1,\n",
    "                        random_state=1337)\n",
    "bagger.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the bagger and the voter we can actually explore the individual models \n",
    "# which have been created using the estimators_ attribute\n",
    "bagger.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that the only difference in the parameterization of the\n",
    "# individual classifier models is the random state.\n",
    "\n",
    "# Since they are all just regular descision trees underneath, we could\n",
    "# do anything we wanted to to the trees, including plotting them\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "fig= plt.figure(figsize=(12,8))\n",
    "plot_tree(bagger.estimators_[0], \n",
    "          feature_names=X_train.columns, \n",
    "          class_names=np.unique(y_train.astype(str)));\n",
    "plt.savefig('tree.svg',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can of course look at the score of the bagger both on training data\n",
    "bagger.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And on our validation data\n",
    "bagger.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Ensemble\n",
    "With boosting our goal is to build an additive model, where our second model builds on the first, then the third model builds on both of those, and so forth. There are various algorithms which can be used to do this, and a common approach in sklearn is to use the `GradientBoostingClassifier`. This method doesn't take a model to use, instead it uses a series of it's own regression trees directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Since this is a tree, we can select various tree parameters. Again, \n",
    "# we can set the n_estimators parameter to determine how many, as\n",
    "# a maximum, trees should be built. You'll notice that there is no\n",
    "# parameter for the number of CPUs -- this approach is a serial one\n",
    "# so you can't easily parallelize this unless you want to train\n",
    "# multiple boosters and ensemble them together\n",
    "booster=GradientBoostingClassifier(min_samples_leaf=7, \n",
    "                                   max_depth=5, \n",
    "                                   n_estimators=100, \n",
    "                                   random_state=1337)\n",
    "\n",
    "booster.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the booster is done we can see how many models it\n",
    "# ended up generating\n",
    "booster.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And like all of our classifiers, we can see the score on the training set\n",
    "booster.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And on our validation data\n",
    "bagger.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble\n",
    "Metalearning consists of using a number of different models and learning which one is best. You can think of this as surveying the talking heads on TV as to who they think is going to perform the best, then learning over time which one to weight more heavily. This means you need to provide both the list of models, as well as a classifier to train to learn which model is best. You can think of it as a weighted intelligent voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# The classifier you use to learn over your individual voters is completely\n",
    "# up to you, but I'm going to go with the default here which is LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "stacker=StackingClassifier(estimators=clfs.items(), \n",
    "                           final_estimator=LogisticRegression(max_iter=1000,random_state=1337), \n",
    "                           cv=5, \n",
    "                           n_jobs=-1)\n",
    "stacker.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can look at the various scores to consider\n",
    "stacker.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can look at the various scores to consider\n",
    "stacker.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles are a powerful way to leverage the benefits of different kinds of models in making accurate predictions. In general, ensembles perform better than individual models, and as long as you keep in mind the issue of data leakage and overfitting, you can expect that in the end you'll want to use these to bring together various models in a team. But, in addition to overfitting, keep in mind the idea of explainability of models. Ensembles make it very difficult to understand why a given prediction is being made, and sometimes a limited depth descision tree or even a logistic regression is a great place to put your efforts."
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9e72876e-9f53-4a2a-952c-aa0d839c094c",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
