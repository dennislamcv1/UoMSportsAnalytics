{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-08e2dda0-4e05-4d68-8304-9dc1b2db2d76",
    "deepnote_cell_type": "markdown",
    "etc_hash": "e76d22796a7aa63ec2c298157389893a88289d89c64bb4feac69eb4640075d1c",
    "tags": []
   },
   "source": [
    "# Tuning and Inspecting Model Trees\n",
    "I've mentioned in passing a few times that we don't have to come up with parameters for building models all by ourselves - we can leverage sklearn's built in functionality for this through a process called `GridSearch`. This method allows you to describe the different parameter values you want to explore, and build individual models for each one. The result is that once you think you have some indication that your features might be reasonable - and I think we have that already - you can fine tune the model through brute computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-9ebd5831-8da9-4ca3-89e8-3aade71e2cd2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "faa473d11575ac023fdcc98bb06b7b6e411f5b2540117124841137b6704e452a",
    "execution_millis": 90179,
    "execution_start": 1622494359228,
    "source_hash": "f6d68135",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's load in our data from the previous lecture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%run m5p.py\n",
    "\n",
    "df=pd.read_csv(\"model_tree_data.csv\")\n",
    "X_train=df[ df.columns.drop('normalized_vote_pct') ]\n",
    "y_train=df['normalized_vote_pct']\n",
    "\n",
    "# GridSearch comes in a cross validation variety, so let's import that\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Now, let's set a few different hyperparameters the M5Prime class can work with\n",
    "# I'm going to choose to explore a few different depths, a few minimum number of\n",
    "# samples per leaf, and a few pruning options\n",
    "parameters={'max_depth':(3,4,5,6), \n",
    "            'min_samples_leaf':(1,3,6),\n",
    "            'use_pruning':[False,True],\n",
    "            }\n",
    "\n",
    "# Now we can just train our model as if it were a regression model directly. Be\n",
    "# aware that this will take a bit of time to run\n",
    "reg=GridSearchCV(estimator=M5Prime(use_smoothing=False), param_grid=parameters, cv=10, scoring='r2')\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-615f99b4-e188-48e7-a742-f574436e5165",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "ca9dec93719e5cb5670596a839b2fed491d60cdc3f838a1fb7b459e3e234a713",
    "execution_millis": 34,
    "execution_start": 1622494449374,
    "source_hash": "91a6fcc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We already know what out naive approach did, with a max_depth=6,\n",
    "# and min_samples_leaf=3, we had an R2 around 0.28. Now we can look at\n",
    "# the best model with the parameters we have tried through grid search\n",
    "reg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-3445da9c-7cfe-4608-a077-eac3e6bf9daa",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "f8f33946606a07cefa3bfcb86c8e91e9c1cb4f60cc0d6b657760b639374c684d",
    "execution_millis": 34,
    "execution_start": 1622494449374,
    "source_hash": "ff0673a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Well, that's not amazingly better. But it's something, we can find lots more\n",
    "# detail about this model in the cv_results_ attribute, including the time and R2\n",
    "# values for every fold and every parameter combination! But If we just want\n",
    "# to see the best set of parameters, the ones which correspond to the score\n",
    "# above, we can get those too.\n",
    "reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-2ef074ef-fa29-40d4-aca1-a1dfe523cfe9",
    "deepnote_cell_type": "markdown",
    "etc_hash": "6bba5621c4a5a93590d27b40ab29079877a7959114fdb2c625c404b40811077f",
    "tags": []
   },
   "source": [
    "Now, there are lots of other investigations we could consider, including expanding our `GridSearchCV` results or changing the regression model we use for leaf nodes. But, I did sort of sell the M5 trees as an interpretable approach to the problem, and maybe we should take a minute to look at that. Recall that our task is to generate a list of NHL players for a given season and the number of votes they will receive for the Hart trophy by journalists. Votes are weighted, and we've chosen to predict the normalized weighting value. We should be able to look at a sorted list of our predictions and compare to our validation year -- the 2018-2019 year. We also should be able to look at our tree and gain some insight into how it comes up with classifications. So let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-0618f067-0891-4b90-81c3-267dc58acdc6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "55235805e6ef474b82dca5da61b34ea4cfa0d243d4a3dc85335b96ce5495a83d",
    "execution_millis": 791,
    "execution_start": 1622494449375,
    "source_hash": "611b8c2e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are a few ways we can look at our model. I'm going to start just by visualizing\n",
    "# the tree as a set of nodes. So let's take our best model, which can be found in\n",
    "# reg.best_estimator_, and predict on our training data\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(reg.best_estimator_, out_file=\"m5p_tree.dot\", \n",
    "                feature_names=X_train.columns, rounded=True, filled=True)\n",
    "\n",
    "# Now we convert this to a PNG for display and load it up here in a new cell\n",
    "!dot -Tpng m5p_tree.dot -o m5p_tree.png\n",
    "from IPython.display import Image\n",
    "display(Image('m5p_tree.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-33fd6839-c47f-4e73-afb2-cf81a5c854de",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "3ca943284aed803e909419d0bc6ed5efa7a6c7b139beec4c6f8c476dfbd20d2a",
    "execution_millis": 8,
    "execution_start": 1622494450158,
    "source_hash": "657c3e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ok, that was a lot to talk about. The tree is just part of the analysis though, we\n",
    "# also have those regression equations at each leaf node. Recall that a regression\n",
    "# equation is a bunch of coefficients, one for each feature, that are effectively a\n",
    "# weighting which when summed together will produce a target value - in this case our\n",
    "# percentage of votes. Now we can get these equations in a few ways, but Sylvain has nicely\n",
    "# included a function which prints out the tree nodes and the linear model equations for\n",
    "# us as well.\n",
    "print(export_text_m5(reg.best_estimator_, out_file=None, node_ids=True))\n",
    "for i,v in enumerate(X_train.columns):\n",
    "    print(f\"{i}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-97a907ca-5127-466c-a219-f84f11c64638",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "7fa2dbd3a6b70d0932b61acf874a78348cd1f73c61210eed19ad0b2d577bd943",
    "execution_millis": 42,
    "execution_start": 1622494450159,
    "source_hash": "b1f089f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ok, let's make this prediction on our holdout! We don't need to retrain\n",
    "# we can just use our best estimator on the data we haven't seen \n",
    "df_validate=pd.read_csv(\"assets/model_tree_holdout_data.csv\")\n",
    "\n",
    "X_validate=df_validate[ df_validate.columns.drop('normalized_vote_pct').drop('fullName') ]\n",
    "y_validate=df_validate['normalized_vote_pct']\n",
    "\n",
    "reg.best_estimator_.score(X_validate,y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "etc_hash": "49d20992bc7ce246c2006b53f20245877f55e795cb7dfd10ec0590818721a64c"
   },
   "source": [
    "Wow, that's a poorly fitting model. How can you even have an $R^2$ value which is negative? Well, what this means is that overall in our regression analysis a constant horizontal line would be a better predictor. That this horizontal line constant would better fit our data than the models we have trained. So, is all hope lost? I don't think so. While we have modeled this problem as a regression problem, our real-world use case is more likely to be something akin to ranking the players who are in the top ten or twents as far as their competitiveness for the Hart Trophy. And this shows some of the gritty challenges in applying machine learning to sports data -- your conceptualization of the problem influences your model and evaluation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "etc_hash": "10b733b308861ea5f54bba3a24539e114353f41df2b783172d3f33ce5d66770d"
   },
   "source": [
    "So, what do we do next? Well, let's actually see how the top ten people in our model compare to the people who received votes in the top ten for that holdout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-aa1dfb2e-f8f1-47e8-a89a-11824da5328b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "f56ffbc37c4bd484aa609a644046102f79dd99ac0be40a5b5f7ff327f58909f8",
    "execution_millis": 55,
    "execution_start": 1622494450193,
    "source_hash": "c6f950a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's embed our prediction data in the df_validate dataframe\n",
    "df_validate[f\"prediction_full_data\"]=reg.best_estimator_.predict(X_validate)\n",
    "\n",
    "# and we'll both sort the values in this dataframe by the prediction and reset our index\n",
    "df_validate=df_validate[['fullName',\"prediction_full_data\"]].sort_values(by=\"prediction_full_data\", ascending=False).reset_index()\n",
    "\n",
    "# Now let's pull down the data from hockey reference\n",
    "# Uncomment to run if not on Coursera\n",
    "# hr=pd.read_html('https://www.hockey-reference.com/awards/voting-2019.html#all-hart-stats')[0]\n",
    "# hr.columns=hr.columns.droplevel(0)\n",
    "# hr=hr[[\"Place\",\"Player\",\"Vote%\"]]\n",
    "# hr.to_csv(\"assets/hocket-reference-20182019.csv\", index=False)\n",
    "hr= pd.read_csv(\"assets/hocket-reference-20182019.csv\")\n",
    "\n",
    "# Now we can just join the two dataframes together on the index and see\n",
    "# how we compare to the actuals\n",
    "df_validate[['fullName',\"prediction_full_data\"]].join(hr,how='inner')"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9058556b-fc45-408b-b81c-2b12551634ca",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
