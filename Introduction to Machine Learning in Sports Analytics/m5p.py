"""
This module gathers methods built arount the M5 (model trees) methodology.
"""

# Authors: Sylvain Marie <sylvain.marie@schneider-electric.com>
#
# License: BSD 3 clause
from collections import namedtuple
from copy import copy
from warnings import warn

import numpy as np
from scipy.sparse import issparse

from sklearn.metrics import mean_squared_error
from sklearn.base import RegressorMixin, clone
from sklearn.base import is_classifier
from sklearn.preprocessing import StandardScaler
from sklearn.utils import check_array
from sklearn.utils.validation import check_is_fitted
from sklearn.linear_model import LinearRegression

from sklearn.tree import _tree
from sklearn.tree._classes import BaseDecisionTree, DTYPE, DOUBLE
#from sklearn.tree._classes_m5_linreg_utils import DeNormalizableMixIn, \
#    linreg_model_to_text, DeNormalizableLinearRegression

__all__ = ["M5Base",
           "M5Prime"]


_SmoothingDetails = namedtuple('_SmoothingDetails', ('A', 'B', 'C'))
"""Internal structure to contain the recursive smoothed coefficients details 
in the smoothing algorithm"""


class M5Base(BaseDecisionTree):
    """
    M5Base. Implements base routines for generating M5 PredictionModel trees and rules.

    The original algorithm M5 was invented by Quinlan:

    Quinlan J. R. (1992). Learning with continuous classes. Proceedings of the
    Australian Joint Conference on Artificial Intelligence. 343--348. World
    Scientific, Singapore.


    Yong Wang made improvements and created M5':

    Wang, Y and Witten, I. H. (1997). Induction of model trees for predicting
    continuous classes. Proceedings of the poster papers of the European
    Conference on Machine Learning. University of Economics, Faculty of
    Informatics and Statistics, Prague.

    Pruning and Smoothing can be activated and deactivated on top of the base
    model. TODO check if 'rules' should be supported too
    
    Inspired by Weka (https://github.com/bnjmn/weka) M5Base class, from Mark Hall
    """

    def __init__(self,
                 criterion='friedman_mse',
                 # According to M5' paper, mse should lead to similar results than (std/rmse), that is not implemented in sklearn.
                 splitter='best',  # take the feature with the best split
                 max_depth=None,  # no maximum depth limit
                 min_samples_split=4,  # in weka this parameter is named "M"
                 min_samples_leaf=2,  # from the M5' article : otherwise (n+v)/(n-v) is infinite
                 min_weight_fraction_leaf=0.,  # TODO this would actually maybe be better than min_sample_leaf ?
                 max_features=None,  # no feature reduction: take all features
                 max_leaf_nodes=None,  # no limitation in number of leaves
                 min_impurity_decrease=0.,
                 # min_impurity_split_as_initial_ratio = 0.05,  # TODO The paper suggests to use 5% of all STDR
                 min_impurity_split=None,
                 random_state=None,
                 class_weight=None,
                 leaf_model=None,  # the regression model used in the leaves (it will be cloned for each leaf)
                 use_pruning=True,
                 use_smoothing=None,  # False, True, or 'installed', or 'on_prediction'. None and True means 'installed' by default except if smoothing constant or smoothing constant ratio is 0.0
                 smoothing_constant=None,  # The number of samples (default=15) that a parent model represents when mixed with a child model. 0 means disable smoothing
                 smoothing_constant_ratio=None,  # if provided, smoothing_constant will not be used. 0.0 means disable smoothing
                 debug_prints=False,
                 ccp_alpha=0.0  # TODO is this relevant ? does that conflict with "use_pruning" ?
                 ):

        # ------ WEKA VERSION FOR REFERENCE ---
        # From https://github.com/bnjmn/weka/blob/master/weka/src/main/java/weka/classifiers/trees/m5/RuleNode.java
        # it is like the M5' paper: a minimum number + a rule on standard deviation ratio
        # if ((m_numInstances < m_splitNum)
        #         | | (Rule.stdDev(m_classIndex, m_instances) < (m_globalDeviation * m_devFraction))) {
        # m_isLeaf = true;
        # Note: their impurity criterion for splitting is a standard deviation reduction (SDR) as stated in the paper:
        # see https://github.com/Waikato/weka-trunk/blob/cd79b7f15ffeb078392aae08f5a4e9ae38620a98/weka/src/main/java/weka/classifiers/trees/m5/Impurity.java
        # it should be equivalent to the mse one

        # ------ THIS (SKLEARN BASE) VERSION ----
        # From https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py
        # max_leaf_nodes = -1 if max_leaf_nodes is None
        # if max_leaf_nodes < 0:
        #     builder = DepthFirstTreeBuilder(splitter, min_samples_split,
        #                                     min_samples_leaf,
        #                                     min_weight_leaf,
        #                                     max_depth,
        #                                     self.min_impurity_decrease,
        #                                     min_impurity_split)

        # From https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx
        # is_leaf = (depth >= max_depth or
        #            n_node_samples < min_samples_split or
        #            n_node_samples < 2 * min_samples_leaf or
        #            weighted_n_node_samples < 2 * min_weight_leaf or
        #            impurity <= min_impurity_split)  (this one is deprecated)
        #
        # if not is_leaf:
        #     DO THE SPLIT :)
        #     splitter.node_split(impurity, & split, & n_constant_features)
        #
        #     is_leaf = (is_leaf or split.pos >= end or
        #                (split.improvement + EPSILON < min_impurity_decrease))

        # TODO the paper suggests to do this with 5%
        # min_impurity_split = min_impurity_split_as_initial_ratio * initial_impurity

        super(M5Base, self).__init__(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            min_impurity_split=min_impurity_split,
            random_state=random_state,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

        # warning : if the field names are different from constructor params,
        # then clone(self) will not work.
        if leaf_model is None:
            # to handle case when the model is learnt on normalized data and
            # we wish to be able to read the model equations.
            leaf_model = DeNormalizableLinearRegression()
        self.leaf_model = leaf_model

        # smoothing related
        if smoothing_constant_ratio is not None and smoothing_constant is not None:
            raise ValueError("Only one of `smoothing_constant` and "
                             "`smoothing_constant_ratio` should be provided")
        elif (smoothing_constant_ratio == 0.0 or smoothing_constant == 0) \
                and (use_smoothing is True or use_smoothing == 'installed'):
            raise ValueError("`use_smoothing` was explicitly enabled with "
                             "pre-installed models, while smoothing "
                             "constant/ratio are explicitly set to zero")

        self.use_pruning = use_pruning
        self.use_smoothing = use_smoothing
        self.smoothing_constant = smoothing_constant
        self.smoothing_constant_ratio = smoothing_constant_ratio

        self.debug_prints = debug_prints

    def as_pretty_text(self, **kwargs):
        """
        Returns a multi-line representation of this decision tree, using 
        `tree_to_text`.

        :return: a multi-line string representing this decision tree
        """
        from ._export_m5p import export_text_m5
        return export_text_m5(self, out_file=None, **kwargs)

    def fit(self, X, y, sample_weight=None, check_input=True,
            X_idx_sorted="deprecated"):

        # (0) smoothing default values behaviour
        if (self.smoothing_constant_ratio == 0.0 
                or self.smoothing_constant == 0):
            self.use_smoothing = False
        elif self.use_smoothing is None or self.use_smoothing is True:
            # default behaviour
            if isinstance(self.leaf_model, LinearRegression):
                self.use_smoothing = 'installed'
            else:
                self.use_smoothing = 'on_prediction'
        # finally make sure we are ok now
        assert self.use_smoothing in (False, np.bool_(False), 'installed',
                                      'on_prediction')

        # (1) Build the initial tree as usual
        super(M5Base, self).fit(
            X, y,
            sample_weight=sample_weight,
            check_input=check_input,
            X_idx_sorted=X_idx_sorted)

        if self.debug_prints:
            print("(debug_prints) Initial tree:")
            print(self.as_pretty_text(node_ids=True))

        # (1b) prune initial tree to take into account min impurity in splits
        # TODO since we're not on sklearn 0.17 anymore this can maybe be
        #  replaced with proper usage of self.min_impurity_split
        prune_on_min_impurity(self.tree_)

        if self.debug_prints:
            print("(debug_prints) Postprocessed tree:")
            print(self.as_pretty_text(node_ids=True))

        # (2) Now prune tree and replace pruned branches with linear models
        # -- unfortunately we have to re-do this input validation
        # step, because it also converts the input to float32.
        if check_input:
            X = check_array(X, dtype=DTYPE, accept_sparse="csc")
            if issparse(X):
                X.sort_indices()

                if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
                    raise ValueError("No support for np.int64 index based "
                                     "sparse matrices")

        # -- initialise the structure to contain the leaves and node models
        self.node_models = np.empty((self.tree_.node_count,), dtype=object)

        # -- Pruning requires to know the global deviation of the target
        global_std_dev = np.nanstd(y)
        # global_abs_dev = np.nanmean(np.abs(y))

        # -- Pruning requires to know the samples that reached each node.
        # From http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html
        # Retrieve the decision path of each sample.
        samples_to_nodes = self.decision_path(X)
        # * row i is to see the nodes (non-empty j) in which sample i appears.
        #   sparse row-first (CSR) format is OK
        # * column j is to see the samples (non-empty i) that fall into that
        #   node. To do that, we need to make it CSC
        nodes_to_samples = samples_to_nodes.tocsc()

        # -- execute the pruning
        # TODO if "self.use_pruning"
        # self.features_usage is a dict feature_idx -> nb times used, only
        #   for used features.
        self.features_usage = build_models_and_get_pruning_info(
            self.tree_, X, y, nodes_to_samples, self.leaf_model,
            self.node_models, global_std_dev, use_pruning=self.use_pruning
        )

        # -- cleanup to compress inner structures: only keep non-pruned ones
        self._cleanup_tree()

        if self.debug_prints:
            print("(debug_prints) Pruned tree:")
            print(self.as_pretty_text(node_ids=True))

        if self.use_smoothing == 'installed':
            # Retrieve the NEW decision path of each sample.
            samples_to_nodes = self.decision_path(X)
            # * row i is to see the nodes (non-empty j) in which sample i
            #   appears. sparse row-first (CSR) format is OK
            # * column j is to see the samples (non-empty i) that fall into
            #   that node. To do that, we need to make it CSC
            nodes_to_samples = samples_to_nodes.tocsc()

            # default behaviour for smoothing constant and ratio
            smoothing_constant = self._get_smoothing_constant_to_use(X)

            self.install_smoothing(X, y, nodes_to_samples,
                                   smoothing_constant=smoothing_constant)

            if self.debug_prints:
                print("(debug_prints) Pruned and smoothed tree:")
                print(self.as_pretty_text(node_ids=True))

        return self

    def _get_smoothing_constant_to_use(self, X):
        """
        Returns the smoothing_constant to use for smoothing, based on current 
        settings and X data
        """
        if self.smoothing_constant_ratio is not None:
            nb_training_samples = X.shape[0]
            smoothing_cstt = self.smoothing_constant_ratio * nb_training_samples
            if smoothing_cstt < 15:
                warn("smoothing constant ratio %s is leading to an extremely "
                     "small smoothing constant %s because nb training samples"
                     " is %s. Clipping to 15."
                     % (self.smoothing_constant_ratio, smoothing_cstt, 
                        nb_training_samples))
                smoothing_cstt = 15
        else:
            smoothing_cstt = self.smoothing_constant
            if smoothing_cstt is None:
                smoothing_cstt = 15  # default from the original Quinlan paper
                
        return smoothing_cstt

    def _cleanup_tree(self):
        """
        Reduces the size of this object by removing from internal structures 
        all items that are not used any more (all leaves that have been pruned)
        """
        old_tree = self.tree_
        old_node_modls = self.node_models

        # Get all information to create a copy of the inner tree. 
        # Note: _tree.copy() is gone so we use the pickle way
        # --- Base info: nb features, nb outputs, output classes
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L631
        # [1] = (self.n_features, sizet_ptr_to_ndarray(self.n_classes, self.n_outputs), self.n_outputs)
        # So these remain, we are just interested in changing the node-related arrays
        new_tree = _tree.Tree(*old_tree.__reduce__()[1])

        # --- Node info
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L637
        dct = old_tree.__getstate__().copy()

        # cleanup: only keep the nodes that are not undefined.
        # note: this is identical to
        # valid_nodes_indices = dct["nodes"]['left_child'] != TREE_UNDEFINED
        valid_nodes_indices = old_tree.children_left != _tree.TREE_UNDEFINED
        # valid_nodes_indices2 = old_tree.children_right != TREE_UNDEFINED
        # assert np.all(valid_nodes_indices == valid_nodes_indices2)
        new_node_count = sum(valid_nodes_indices)

        # create empty new structures
        n_shape = (new_node_count, *dct["nodes"].shape[1:])
        new_nodes = _empty_contig_ar(n_shape, dtype=dct["nodes"].dtype)
        v_shape = (new_node_count, *dct["values"].shape[1:])
        new_values = _empty_contig_ar(v_shape, dtype=dct["values"].dtype)
        m_shape = (new_node_count, *old_node_modls.shape[1:])
        new_node_models = _empty_contig_ar(m_shape, dtype=old_node_modls.dtype)

        # Fill structures while reindexing the tree and remembering the depth
        global next_free_id
        next_free_id = 0

        def _compress(old_node_id):
            """

            :param old_node_id:
            :return: the depth and new indices of left and right children
            """
            global next_free_id
            new_node_id = next_free_id
            next_free_id += 1

            # use the old tree to walk
            old_node = dct["nodes"][old_node_id]
            left_id = old_node['left_child']
            right_id = old_node['right_child']

            # Create the new node with a copy of the old
            new_nodes[new_node_id] = old_node  # this is an entire row so it is probably copied already by doing so.
            new_values[new_node_id] = dct["values"][old_node_id]
            new_node_models[new_node_id] = copy(old_node_modls[old_node_id])

            if left_id == _tree.TREE_LEAF:
                # ... and right_id == _tree.TREE_LEAF
                # => node_id is a leaf. Nothing to do
                return 1, new_node_id
            else:

                left_depth, new_id_left = _compress(left_id)
                right_depth, new_id_right = _compress(right_id)

                # store the new indices
                new_nodes[new_node_id]['left_child'] = new_id_left
                new_nodes[new_node_id]['right_child'] = new_id_right

                return 1 + max(left_depth, right_depth), new_node_id

        # complete definition of the new tree
        dct["max_depth"] = _compress(0)[0] - 1  # root node has depth 0, not 1
        dct["node_count"] = new_node_count  # new_nodes.shape[0]
        dct["nodes"] = new_nodes
        dct["values"] = new_values
        new_tree.__setstate__(dct)

        # Fix an issue on sklearn 0.17.1: setstate was not updating max_depth
        # see https://github.com/scikit-learn/scikit-learn/blob/0.17.1/sklearn/tree/_tree.pyx#L623
        new_tree.max_depth = dct["max_depth"]

        # update self fields
        self.tree_ = new_tree
        self.node_models = new_node_models

    def install_smoothing(
            self, 
            X_train_all, y_train_all, nodes_to_samples, smoothing_constant
    ):
        """
        Executes the smoothing procedure described in the M5 and M5P paper, 
        "once for all". This means that all models are modified so that after 
        this method has completed, each model in the tree is already a smoothed
        model.

        This has pros (prediction speed) and cons (the model equations are 
        harder to read - lots of redundancy)

        It can only be done if leaf models are instances of `LinearRegression`
        """
        # check validity: leaf models have to support pre-computed smoothing
        if not isinstance(self.leaf_model, LinearRegression):
            raise TypeError("`install_smoothing` is only available if leaf "
                            "models are instances of `LinearRegression` or a "
                            "subclass")

        # Select the Error metric to compute model errors (used to compare 
        # node with subtree for pruning)
        # TODO in Weka they use RMSE, but in papers they use MAE. this could be a parameter.
        # TODO Shall we go further and store the residuals, or a bunch of metrics? not sure
        err_metric = root_mean_squared_error  # mean_absolute_error

        # --- Node info
        # TODO we should probably not do this once in each method, but once or give access directly (no copy)
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L637
        dct = self.tree_.__getstate__().copy()
        old_node_models = self.node_models

        m_shape = old_node_models.shape
        new_node_models = _empty_contig_ar(m_shape, dtype=old_node_models.dtype)

        def smooth__(coefs_,
                     n_samples,
                     features=None,
                     parent=None,  # type: _SmoothingDetails
                     parent_features=None,
                     k=smoothing_constant  #
                     ):
            # type: (...) -> _SmoothingDetails
            """
            Smoothes the model coefficients or intercept `coefs_` (an array or
            a scalar), using the smoothing results at parent node.
            At each node we keep in memory three results A, B, C.

             - the new coef at each node is A + B
             - the recursion equations are
                - B(n) = (k / (n_samples(n) + k)) * A(n-1) + B(n-1)
                - C(n) = (n_samples(n) / (n_samples(n) + k)) * C(n-1)
                - A(n) = coef(n)*C(n)
            """
            if parent is None:
                # A0 = coef(0), C0 = 1, B0 = 0
                if features is None:
                    # single scalar value
                    return _SmoothingDetails(A=coefs_, B=0, C=1)
                else:
                    # vector
                    if len(coefs_) == 0:
                        coefs_ = np.asarray(coefs_)
                    if coefs_.shape[0] != len(features):
                        raise ValueError("nb features does not match the nb "
                                         "of coefficients")
                    return _SmoothingDetails(
                        A=coefs_, B=np.zeros(coefs_.shape, dtype=coefs_.dtype),
                        C=np.ones(coefs_.shape, dtype=coefs_.dtype)
                    )
            else:
                # B(n) = k/(n_samples(n)+k)*A(n-1) + B(n-1)
                # C(n) = (n_samples(n) / (n_samples(n) + k)) * C(n-1)
                Bn = (k / (n_samples + k)) * parent.A + parent.B
                Cn = (n_samples / (n_samples + k)) * parent.C

                # A(n) = coef(n) * C(n)
                if features is None:
                    # single scalar value: easy
                    An = coefs_ * Cn
                    return _SmoothingDetails(A=An, B=Bn, C=Cn)
                else:
                    # vector of coefs: we have to 'expand' the coefs array
                    # because the coefs at this node apply for (features)
                    # while coefs at parent node apply for (parent_features)
                    An = np.zeros(Cn.shape, dtype=Cn.dtype)
                    parent_features = np.array(parent_features)
                    features = np.array(features)

                    # thanks https://stackoverflow.com/a/8251757/7262247 !
                    index = np.argsort(parent_features)
                    sorted_parents = parent_features[index]
                    sorted_index = np.searchsorted(sorted_parents, features)

                    features_index = np.take(index, sorted_index, mode="clip")
                    if np.any(parent_features[features_index] != features):
                        # result = np.ma.array(features_index, mask=mask)
                        raise ValueError("Internal error - please report this."
                                         "One feature was found in the child "
                                         "node, that was not in the parent "
                                         "node.")

                    if len(features_index) > 0:
                        An[features_index] = coefs_ * Cn[features_index]

                    # old inefficient version
                    # for i, feature in enumerate(parent_features):
                    #     try:
                    #         j = features.index(feature)
                    #     except IndexError:
                    #         pass
                    #     else:
                    #         An[i] = coefs_[features[j]] * Cn[i]

                    return _SmoothingDetails(A=An, B=Bn, C=Cn)

        def _smooth(node_id, parent_features=None,
                    parent_coefs=None,     # type: _SmoothingDetails
                    parent_intercept=None  # type: _SmoothingDetails
                    ):
            # gather all info on this node
            # --base regression tree
            node_info = dct["nodes"][node_id]
            left_id = node_info['left_child']
            right_id = node_info['right_child']
            # --additional model
            node_model = old_node_models[node_id]
            # --samples
            samples_at_this_node = get_samples_at_node(node_id,
                                                       nodes_to_samples)
            n_samples_at_this_node = samples_at_this_node.shape[0]
            # note: should be equal to tree.n_node_samples[node_id]
            assert n_samples_at_this_node == self.tree_.n_node_samples[node_id]
            y_true_this_node = y_train_all[samples_at_this_node]
            X_this_node = X_train_all[samples_at_this_node, :]

            # (1) smooth current node
            # parent_model = old_node_modls[parent_id] if parent_id is not None
            parent_features = parent_features if parent_features is not None else None
            is_constant_leaf = False
            if (left_id == _tree.TREE_LEAF
                    and isinstance(node_model, ConstantLeafModel)):
                is_constant_leaf = True
                node_features = ()
                smoothed_features = (parent_features
                                     if parent_features is not None
                                     else node_features)
                node_coefs = ()
                node_intercept = dct["values"][node_id]
                # extract the unique scalar value
                assert len(node_intercept) == 1
                node_intercept = node_intercept.item()
            else:
                # a leaf LinRegLeafModel or a split SplitNodeModel
                node_features = node_model.features
                smoothed_features = (parent_features
                                     if parent_features is not None
                                     else node_features)
                node_coefs = node_model.model.coef_
                node_intercept = node_model.model.intercept_

            # create a new linear regression model with smoothed coefficients
            smoothed_sklearn_model = clone(self.leaf_model)
            smoothed_coefs = smooth__(node_coefs, features=node_features,
                                      n_samples=n_samples_at_this_node,
                                      parent=parent_coefs,
                                      parent_features=parent_features)
            smoothed_intercept = smooth__(node_intercept,
                                          n_samples=n_samples_at_this_node,
                                          parent=parent_intercept)
            smoothed_sklearn_model.coef_ = smoothed_coefs.A + smoothed_coefs.B
            smoothed_sklearn_model.intercept_ = (smoothed_intercept.A
                                                 + smoothed_intercept.B)

            # finally update the node
            if is_constant_leaf:
                smoothed_node_model = LinRegLeafModel(
                    smoothed_features, smoothed_sklearn_model, None
                )
            else:
                smoothed_node_model = copy(node_model)
                smoothed_node_model.features = smoothed_features
                smoothed_node_model.model = smoothed_sklearn_model

            # remember the new smoothed model
            new_node_models[node_id] = smoothed_node_model

            if left_id == _tree.TREE_LEAF:
                # If this is a leaf, update the prediction error on X
                y_pred_this_node = smoothed_node_model.predict(X_this_node)
                smoothed_node_model.error = err_metric(
                    y_true_this_node, y_pred_this_node
                )

            else:
                # If this is a split node - recurse on each subtree
                _smooth(left_id, parent_features=smoothed_features,
                        parent_coefs=smoothed_coefs,
                        parent_intercept=smoothed_intercept)
                _smooth(right_id, parent_features=smoothed_features,
                        parent_coefs=smoothed_coefs,
                        parent_intercept=smoothed_intercept)

                # Update the error using the same formula than the one we used
                # in build_models_and_get_pruning_info
                y_pred_children = predict_from_leaves_no_smoothing(
                    self.tree_, new_node_models, X_this_node
                )
                err_children = err_metric(y_true_this_node, y_pred_children)

                # the total number of parameters is the sum of params in each
                # branch PLUS 1 for the split
                n_params_splitmodel = (new_node_models[left_id].n_params
                                       + new_node_models[right_id].n_params
                                       + 1)
                smoothed_node_model.n_params = n_params_splitmodel
                # do not adjust the error now, simply store the raw one
                # smoothed_node_model.error = compute_adjusted_error(
                #    err_children, n_samples_at_this_node, n_params_splitmodel)
                smoothed_node_model.error = err_children

            return

        # smooth the whole tree
        _smooth(0)

        # use the new node models now
        self.node_models = new_node_models

        # remember the smoothing constant installed
        self.installed_smoothing_constant = smoothing_constant
        return

    def denormalize(self, x_scaler, y_scaler):
        """
        De-normalizes this model according to the provided x and y normalization scalers.
        Currently only StandardScaler issupported.

        :param x_scaler: a StandardScaler or None
        :param y_scaler: a StandardScaler or None
        :return:
        """
        # perform denormalization
        self._denormalize_tree(x_scaler, y_scaler)

    def _denormalize_tree(self, x_scaler, y_scaler):
        """
        De-normalizes all models in the tree
        :return:
        """
        old_tree = self.tree_
        old_node_models = self.node_models

        # Get all information to create a copy of the inner tree.
        # Note: _tree.copy() is gone so we use the pickle way
        # --- Base info: nb features, nb outputs, output classes
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L631
        # [1] = (self.n_features, sizet_ptr_to_ndarray(self.n_classes, self.n_outputs), self.n_outputs)
        # So these remain, we are just interested in changing the node-related arrays
        new_tree = _tree.Tree(*old_tree.__reduce__()[1])

        # --- Node info
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L637
        dct = old_tree.__getstate__().copy()

        # create empty new structures for nodes and models
        n_shape = dct["nodes"].shape
        new_nodes = _empty_contig_ar(n_shape, dtype=dct["nodes"].dtype)
        v_shape = dct["values"].shape
        new_values = _empty_contig_ar(v_shape, dtype=dct["values"].dtype)
        m_shape = old_node_models.shape
        new_node_models = _empty_contig_ar(m_shape, dtype=old_node_models.dtype)

        def _denormalize(node_id, x_scaler, y_scaler):
            """
            denormalizes the subtree below node `node_id`.

             - `new_nodes[node_id]` is filled with a copy of the old node
               (see old_node.dtype to see the various fields). If the node is
               a split node, the split threshold
               `new_nodes[node_id]['threshold']` is denormalized.

             - `new_values[node_id]` is filled with a denormalized copy of the
                old constant prediction at this node. Reminder: this constant
                prediction is actually used only when on the leaves now, but
                we keep it for ref.

             - `new_node_models[node_id]` is filled with a copy of the old
               model `old_node_models[node_id]`, that is denormalized if it
               is not a constant leaf

            :param node_id:
            :return: (nothing)
            """
            # use the old tree to walk
            old_node = dct["nodes"][node_id]
            left_id = old_node['left_child']
            right_id = old_node['right_child']

            # Create the new node with a copy of the old
            new_nodes[node_id] = old_node  # this is an entire row so it is probably copied already by doing so.
            new_model = copy(old_node_models[node_id])
            new_node_models[node_id] = new_model

            # Create the new value by de-scaling y
            if y_scaler is not None:
                # Note: if this is a split node with a linear regression model
                # the value will never be used. However to preserve consistency
                # of the whole values structure and for debugging purposes, we
                # choose this safe path of denormalizing ALL.
                # TODO we could also do it at once outside of the recursive
                #  calls, but we should check for side effects
                new_values[node_id] = y_scaler.inverse_transform(
                    dct["values"][node_id]
                )
            else:
                # no denormalization: simply copy
                new_values[node_id] = dct["values"][node_id]

            if left_id == _tree.TREE_LEAF:
                # ... and right_id == _tree.TREE_LEAF
                # => node_id is a leaf
                if isinstance(new_model, ConstantLeafModel):
                    # nothing to do: we already re-scaled the value
                    return
                elif isinstance(new_model, LinRegLeafModel):
                    # denormalize model
                    new_model.denormalize(x_scaler, y_scaler)
                    return
                else:
                    raise TypeError("Internal error - Leafs can only be"
                                    "constant or linear regression")
            else:
                # this is a split node, denormalize each subtree
                _denormalize(left_id, x_scaler, y_scaler)
                _denormalize(right_id, x_scaler, y_scaler)

                # denormalize the split value if needed
                if x_scaler is not None:
                    split_feature = old_node['feature']
                    # The denormalizer requires a vector with all the features,
                    # even if we only want to denormalize one.
                    # -- put split value in a vector where it has pos 'feature'
                    old_threshold_and_zeros = np.zeros((self.n_features_, ), dtype=dct["nodes"]['threshold'].dtype)
                    old_threshold_and_zeros[split_feature] = old_node['threshold']
                    # -- denormalize the vector and retrieve value 'feature'
                    new_nodes[node_id]['threshold'] = x_scaler.inverse_transform(old_threshold_and_zeros)[split_feature]
                else:
                    # no denormalization: simply copy
                    new_nodes[node_id]['threshold'] = old_node['threshold']

                if isinstance(new_model, SplitNodeModel):
                    # denormalize model at split node too, even if it is not
                    # always used (depending on smoothing mode)
                    new_model.denormalize(x_scaler, y_scaler)
                else:
                    raise TypeError("Internal error: all intermediate nodes"
                                    "should be SplitNodeModel")

                return

        # denormalize the whole tree and put the result in (new_nodes,
        # new_values, new_node_models) recursively
        _denormalize(0, x_scaler, y_scaler)

        # complete definition of the new tree
        dct["nodes"] = new_nodes
        dct["values"] = new_values
        new_tree.__setstate__(dct)

        # update the self fields
        # self.features_usage
        self.tree_ = new_tree
        self.node_models = new_node_models

    def compress_features(self):
        """
        Compresses the model and returns a vector of required feature indices.
        This model input will then be X[:, features] instead of X.
        """
        used_features = sorted(self.features_usage.keys())
        new_features_lookup_dct = {old_feature_idx: i
                                   for i, old_feature_idx
                                   in enumerate(used_features)}

        if used_features == list(range(self.n_features_)):
            # NOTHING TO DO: we need all features
            return used_features

        # Otherwise, We can compress. For this we have to create a copy of the
        # tree because we will change its internals
        old_tree = self.tree_
        old_node_modls = self.node_models

        # Get all information to create a copy of the inner tree.
        # Note: _tree.copy() is gone so we use the pickle way
        # --- Base info: nb features, nb outputs, output classes
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L631
        # [1] = (self.n_features, sizet_ptr_to_ndarray(self.n_classes, self.n_outputs), self.n_outputs)
        # So these remain, we are just interested in changing the node-related arrays
        new_tree = _tree.Tree(*old_tree.__reduce__()[1])

        # --- Node info
        # see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L637
        dct = old_tree.__getstate__().copy()

        # create empty new structures for nodes and models
        n_shape = dct["nodes"].shape
        new_nodes = _empty_contig_ar(n_shape, dtype=dct["nodes"].dtype)
        m_shape = old_node_modls.shape
        new_node_models = _empty_contig_ar(m_shape, dtype=old_node_modls.dtype)

        def _compress_features(node_id):
            """

            :param node_id:
            :return: the depth and new indices of left and right children
            """
            # use the old tree to walk
            old_node = dct["nodes"][node_id]
            left_id = old_node['left_child']
            right_id = old_node['right_child']

            # Create the new node with a copy of the old
            new_nodes[node_id] = old_node  # this is an entire row so it is probably copied already by doing so.
            new_model = copy(old_node_modls[node_id])
            new_node_models[node_id] = new_model

            if left_id == _tree.TREE_LEAF:
                # ... and right_id == _tree.TREE_LEAF
                # => node_id is a leaf
                if isinstance(new_model, ConstantLeafModel):
                    # no features used
                    return
                elif isinstance(new_model, LinRegLeafModel):
                    # compress that model
                    new_model.reindex_features(new_features_lookup_dct)
                    return
                else:
                    raise TypeError("Internal error - Leafs can only be"
                                    "constant or linear regression")
            else:

                _compress_features(left_id)
                _compress_features(right_id)

                # store the new split feature index in the node
                new_nodes[node_id]['feature'] = (
                    new_features_lookup_dct[old_node['feature']]
                )

                if isinstance(new_model, SplitNodeModel):
                    # TODO now that split node models have a linear regression
                    #  model too, we should update them here
                    pass
                else:
                    raise TypeError("Internal error: all intermediate nodes"
                                    "should be SplitNodeModel")

                return

        _compress_features(0)

        # complete definition of the new tree
        dct["nodes"] = new_nodes
        new_tree.__setstate__(dct)

        # update the self fields
        self.features_usage = {new_features_lookup_dct[k]: v
                               for k, v in self.features_usage.items()}
        self.tree_ = new_tree
        self.node_models = new_node_models
        self.n_features_ = len(self.features_usage)

        # return the vector of used features
        return used_features

    @property
    def feature_importances_(self):
        """Return the feature importances (the higher, the more important).

        Returns
        -------
        feature_importances_ : array, shape = [n_features]
        """
        check_is_fitted(self, 'tree_')

        # TODO when m5tree inherits from tree,compute_feature_importances below
        features = np.array([self.features_usage[k]
                             for k in sorted(self.features_usage.keys())],
                            dtype=int)

        return features

    # def compute_feature_importances(self, normalize=True):
    #     """Computes the importance of each feature (aka variable)."""
    #     nodes = self.nodes
    #     node = nodes
    #     end_node = node + self.node_count
    #
    #     normalizer = 0.
    #
    #     np.ndarray[np.float64_t, ndim=1] importances
    #     importances = np.zeros((self.n_features,))
    #     cdef DOUBLE_t* importance_data = <DOUBLE_t*>importances.data
    #
    #     with nogil:
    #         while node != end_node:
    #             if node.left_child != _TREE_LEAF:
    #                 # ... and node.right_child != _TREE_LEAF:
    #                 left = &nodes[node.left_child]
    #                 right = &nodes[node.right_child]
    #
    #                 importance_data[node.feature] += (
    #                     node.weighted_n_node_samples * node.impurity -
    #                     left.weighted_n_node_samples * left.impurity -
    #                     right.weighted_n_node_samples * right.impurity)
    #             node += 1
    #
    #     importances /= nodes[0].weighted_n_node_samples
    #
    #     if normalize:
    #         normalizer = np.sum(importances)
    #
    #         if normalizer > 0.0:
    #             # Avoid dividing by zero (e.g., when root is pure)
    #             importances /= normalizer
    #
    #     return importances

    def predict(self, X, check_input=True, smooth_predictions=None,
                smoothing_constant=None):
        """Predict class or regression value for X.

        For a classification model, the predicted class for each sample in X is
        returned. For a regression model, the predicted value based on X is
        returned.

        Parameters
        ----------
        X : array-like or sparse matrix of shape = [n_samples, n_features]
            The input samples. Internally, it will be converted to
            ``dtype=np.float32`` and if a sparse matrix is provided
            to a sparse ``csr_matrix``.

        check_input : boolean, (default=True)
            Allow to bypass several input checking.
            Don't use this parameter unless you know what you do.

        smooth_predictions : boolean, (default=None)
            None means "use self config"

        smoothing_constant: int, (default=self.smoothing_constant)
            Smoothing constant used when smooth_predictions is True. During
            smoothing, child node models are recursively mixed with their
            parent node models, and the mix is done using a weighted sum. The
            weight given to the child model is the number of training samples
            that reached its node, while the weight given to the parent model
            is the smoothing constant. Therefore it can be seen as an
            equivalent number of samples that parent models represent when
            injected into the recursive weighted sum.

        Returns
        -------
        y : array of shape = [n_samples] or [n_samples, n_outputs]
            The predicted classes, or the predict values.
        """
        check_is_fitted(self, 'tree_')

        # If this is just a constant node only check the input's shape.
        if self.n_features_ == 0:
            # perform the input checking manually to set ensure_min_features=0
            X = check_array(X, dtype=DTYPE, accept_sparse="csr",
                            ensure_min_features=0)
            if issparse(X) and (X.indices.dtype != np.intc or
                                X.indptr.dtype != np.intc):
                raise ValueError("No support for np.int64 index based "
                                 "sparse matrices")
            # skip it then
            check_input = False

        # validate and convert dtype
        X = self._validate_X_predict(X, check_input)

        # -------- This is the only change wrt parent class. TODO maybe rather replace self.tree_ with a proxy ---------
        if smooth_predictions is None:
            # Default: smooth prediction at prediction time if configured as
            # such in the model. Note: if self.use_smoothing == 'installed',
            # models are already smoothed models, so no need to smooth again
            smooth_predictions = (self.use_smoothing == 'on_prediction')
        else:
            # user provided an explicit value for smooth_predictions
            if not smooth_predictions and self.use_smoothing == 'installed':
                raise ValueError("Smoothing has been pre-installed on this "
                                 "tree, it is not anymore possible to make "
                                 "predictions without smoothing")

        if smooth_predictions and smoothing_constant is None:
            # default parameter for smoothing is the one defined in the model
            # (with possible ratio)
            smoothing_constant = self._get_smoothing_constant_to_use(X)

        # Do not use the embedded tree (like in super): it has been pruned but
        # still has constant nodes
        # proba = self.tree_.predict(X)
        if not smooth_predictions:
            proba = predict_from_leaves_no_smoothing(self.tree_,
                                                     self.node_models, X)
        else:
            proba = predict_from_leaves(self, X, smoothing=True,
                                        smoothing_constant=smoothing_constant)
        if len(proba.shape) < 2:
            proba = proba.reshape(-1, 1)
        # ------------------------------------------

        n_samples = X.shape[0]

        # Classification
        if is_classifier(self):
            if self.n_outputs_ == 1:
                return self.classes_.take(np.argmax(proba, axis=1), axis=0)
            else:
                predictions = np.zeros((n_samples, self.n_outputs_))
                for k in range(self.n_outputs_):
                    predictions[:, k] = self.classes_[k].take(
                        np.argmax(proba[:, k], axis=1),
                        axis=0)
                return predictions

        # Regression
        else:
            if self.n_outputs_ == 1:
                return proba[:, 0]
            else:
                return proba[:, :, 0]


class ConstantLeafModel:
    """
    Represents the additional information about a leaf node that is not pruned.
    It contains the error associated with the training samples at this node.

    Note: the constant value is not stored here, as it is already available in
    the sklearn tree struct. So to know the prediction at this node, use
    `tree.value[node_id]`
    """
    __slots__ = ('error',)

    def __init__(self, error):
        self.error = error

    # @property
    # def n_drivers(self):
    #     """
    #     Returns the number of drivers used by this model. (This does not
    #     include the constant driver)
    #     """
    #     return 0

    @property
    def n_params(self):
        """
        Returns the number of parameters used by this model, including the
        constant driver
        """
        return 1

    @staticmethod
    def predict_cstt(tree, node_id, n_samples):
        """
        This static method is a helper to get an array of constant predictions
        associated with this node.

        :param tree:
        :param node_id:
        :param n_samples:
        :return:
        """
        cstt_prediction = tree.value[node_id]

        # cstt_prediction can be multioutput so it is an array. replicate it
        from numpy import matlib  # note: np.matlib not available in 1.10.x
        return matlib.repmat(cstt_prediction, n_samples, 1)

from abc import abstractmethod, ABCMeta

import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.linear_model._base import LinearModel
from sklearn.preprocessing import StandardScaler
from sklearn.utils.extmath import safe_sparse_dot


def linreg_model_to_text(model, feature_names=None, target_name=None,
                         precision=3, line_breaks=False):
    """
    Converts a linear regression model to a text representation.
    :param model:
    :param feature_names:
    :param target_name:
    :param precision:
    :param line_breaks: if True, each term in the sum shows in a different line
    :return:
    """
    bits = []

    # Template for numbers: we want scientific notation with a given precision
    nb_tpl = "%%0.%se" % precision

    # Handle multi-dimensional y (its second dim must be size 1, though)
    if len(model.coef_.shape) > 1:
        assert model.coef_.shape[0] == 1
        assert len(model.coef_.shape) == 2
        coefs = np.ravel(model.coef_)  # convert to 1D
        assert len(model.intercept_) == 1
        intercept = model.intercept_.item()  # extract scalar
    else:
        coefs = model.coef_           # a 1D array
        intercept = model.intercept_  # a scalar already

    # First all coefs * drivers
    for i, c in enumerate(coefs):
        var_name = ("X[%s]" % i) if feature_names is None else feature_names[i]

        if i == 0:
            # first term
            if c < 1:
                # use scientific notation
                product_text = (nb_tpl + " * %s") % (c, var_name)
            else:
                # use standard notation with precision
                c = np.round(c, precision)
                product_text = "%s * %s" % (c, var_name)
        else:
            # all the other terms: the sign should appear
            lb = '\n' if line_breaks else ""
            coef_abs = np.abs(c)
            coef_sign = '+' if np.sign(c) > 0 else '-'
            if coef_abs < 1:
                # use scientific notation
                product_text = (("%s%s " + nb_tpl + " * %s")
                                % (lb, coef_sign, coef_abs, var_name))
            else:
                # use standard notation with precision
                coef_abs = np.round(coef_abs, precision)
                product_text = ("%s%s %s * %s"
                                % (lb, coef_sign, coef_abs, var_name))

        bits.append(product_text)

    # Finally the intercept
    if len(bits) == 0:
        # intercept is the only term in the sum
        if intercept < 1:
            # use scientific notation only for small numbers (otherwise 12
            # would read 1.2e1 ... not friendly)
            constant_text = nb_tpl % intercept
        else:
            # use standard notation with precision
            i = np.round(intercept, precision)
            constant_text = "%s" % i
    else:
        # there are other terms in the sum: the sign should appear
        lb = '\n' if line_breaks else ""
        coef_abs = np.abs(intercept)
        coef_sign = '+' if np.sign(intercept) > 0 else '-'
        if coef_abs < 1:
            # use scientific notation
            constant_text = ("%s%s " + nb_tpl) % (lb, coef_sign, coef_abs)
        else:
            # use standard notation with precision
            coef_abs = np.round(coef_abs, precision)
            constant_text = "%s%s %s" % (lb, coef_sign, coef_abs)

    bits.append(constant_text)

    txt = " ".join(bits)
    if target_name is not None:
        txt = target_name + " = " + txt

    return txt


class DeNormalizableMixIn(metaclass=ABCMeta):
    """
    An abstract class that models able to de-normalize should implement.
    """
    __slots__ = ()

    @abstractmethod
    def denormalize(self,
                    x_scaler: StandardScaler = None,
                    y_scaler: StandardScaler = None
                    ):
        """
        Denormalizes the model, knowing that it was fit with the given
        x_scaler and y_scaler
        """


class DeNormalizableLinearModelMixIn(DeNormalizableMixIn, LinearModel):
    """
    A mix-in class to add 'denormalization' capability to a linear model
    """
    def denormalize(self,
                    x_scaler: StandardScaler = None,
                    y_scaler: StandardScaler = None
                    ):
        """
        De-normalizes the linear regression model.
        Before this function is executed,
            (y-y_mean)/y_scale = self.coef_.T <dot> (x-x_mean)/x_scale + self.intercept_
        so
            (y-y_mean)/y_scale = (self.coef_/x_scale).T <dot> x + (self.intercept_ - self.coef_.T <dot> x_mean/x_scale)
        that is
            (y-y_mean)/y_scale = new_coef.T <dot> x + new_intercept
        where
         * new_coef = (self.coef_/x_scale)
         * new_intercept = (self.intercept_ - (self.intercept_ - self.coef_.T <dot> x_mean/x_scale)
        Then going back to y
            y = (new_coef * y_scale).T <dot> x + (new_intercept * y_scale + y_mean)
        :param self:
        :param x_scaler:
        :param y_scaler:
        :return:
        """
        # First save old coefficients
        self.normalized_coef_ = self.coef_
        self.normalized_intercept_ = self.intercept_

        # denormalize coefficients to take into account the x normalization
        if x_scaler is not None:
            new_coef = self.coef_ / x_scaler.scale_
            new_intercept = (
                    self.intercept_ -
                    safe_sparse_dot(x_scaler.mean_, new_coef.T,
                                    dense_output=True)
            )

            self.coef_ = new_coef
            self.intercept_ = new_intercept

        # denormalize them further to take into account the y normalization
        if y_scaler is not None:
            new_coef = self.coef_ * y_scaler.scale_
            new_intercept = y_scaler.inverse_transform(
                np.atleast_1d(self.intercept_)
            )
            if np.isscalar(self.intercept_):
                new_intercept = new_intercept[0]
            self.coef_ = new_coef
            self.intercept_ = new_intercept


class DeNormalizableLinearRegression(LinearRegression,
                                     DeNormalizableLinearModelMixIn):
    """
    A Denormalizable linear regression. The old normalized coefficients are
    kept in a new field named `feature_importances_`
    """
    @property
    def feature_importances_(self):
        if hasattr(self, '_feature_importances_'):
            return self._feature_importances_
        else:
            return self.coef_

    def denormalize(self,
                    x_scaler: StandardScaler = None,
                    y_scaler: StandardScaler = None):
        """
        Denormalizes the model, and saves a copy of the old normalized
        coefficients in self._feature_importances.
        :param x_scaler:
        :param y_scaler:
        :return:
        """
        self._feature_importances_ = self.coef_
        super(DeNormalizableLinearRegression, self).denormalize(x_scaler,
                                                                y_scaler)


class LinRegNodeModel(DeNormalizableMixIn):
    """
    Represents the additional information about a tree node that contains a
    linear regression model.

    It contains
     - the features used by the model,
     - the scikit learn model object itself,
     - and the error for the training samples that reached this node

    """
    __slots__ = ('features', 'model', 'error')

    def __init__(self, features, model, error):
        self.features = features
        self.model = model
        self.error = error

    def to_text(self, feature_names=None, target_name=None, precision=3,
                line_breaks=False):
        """ Returns a text representation of the linear regression model """
        return linreg_model_to_text(self.model, feature_names=feature_names,
                                    target_name=target_name,
                                    precision=precision,
                                    line_breaks=line_breaks)

    @property
    def n_params(self):
        """
        Returns the number of parameters used by this model, including the
        constant driver
        """
        return len(self.features) + 1

    def reindex_features(self, new_features_lookup_dct):
        """
        Reindexes the required features using the provided lookup dictionary.

        :param new_features_lookup_dct:
        :return:
        """
        self.features = [new_features_lookup_dct[f] for f in self.features]

    def denormalize(self,
                    x_scaler: StandardScaler=None,
                    y_scaler: StandardScaler=None
                    ):
        """
        De-normalizes the linear model.

        :param x_scaler:
        :param y_scaler:
        :return:
        """
        # create a clone of the x scaler with only the used features
        x_scaler = copy(x_scaler)
        if len(self.features) > 0:
            x_scaler.scale_ = x_scaler.scale_[self.features]
            x_scaler.mean_ = x_scaler.mean_[self.features]
        else:
            # in that particular case, the above expression is not working
            x_scaler.scale_ = x_scaler.scale_[0:0]
            x_scaler.mean_ = x_scaler.mean_[0:0]

        # use the denormalize function on the internal model
        # TODO what if internal model is not able ?
        self.model.denormalize(x_scaler, y_scaler)

    def predict(self, X):
        """ Performs a prediction for X, only using the required features """
        if len(self.features) < 1 and isinstance(self.model, LinearRegression):
            # unfortunately LinearRegression models do not like it when no
            # features are needed: their input validation requires at least 1.
            # so we do it ourselves.
            return self.model.intercept_ * np.ones((X.shape[0], 1))
        else:
            return self.model.predict(X[:, self.features])


class LinRegLeafModel(LinRegNodeModel):
    """
    Represents the additional information about a leaf node with a linear
    regression model
    """
    pass


class SplitNodeModel(LinRegNodeModel):
    """
    Represents the additional information about a split node, with a linear
    regression model.
    """
    __slots__ = ('n_params', )

    def __init__(self, n_params, error, features, model):
        self.n_params = n_params
        super(SplitNodeModel, self).__init__(features=features, model=model,
                                             error=error)


PRUNING_MULTIPLIER = 2
# see https://github.com/bnjmn/weka/blob/master/weka/src/main/java/weka/classifiers/trees/m5/RuleNode.java#L124
# TODO check why they use 2 instead of 1 (from the article) ??


def root_mean_squared_error(*args, **kwargs):
    return np.sqrt(mean_squared_error(*args, **kwargs))


def prune_on_min_impurity(tree):
    """
    Edits the given tree so as to prune subbranches that do not respect the min
    impurity criterion

    The paper suggests to do this with 5% but min_impurity_split is not
    available in 0.17 (and criterion is not std but mse so we have to square)

    :param tree:
    :return:
    """
    left_children = tree.children_left
    right_children = tree.children_right
    impurities = tree.impurity

    # The paper suggests to do this with 5% but min_impurity_split is not
    # available in 0.17 (and criterion is not std but mse so we have to square)
    root_impurity = impurities[0]
    impurity_threshold = root_impurity * (0.05 ** 2)

    def stop_on_min_impurity(node_id):
        # note: in the paper that is 0.05 but criterion is on std. Here
        # impurity is mse so a squared equivalent of std.
        left_id = left_children[node_id]
        right_id = right_children[node_id]
        if left_id != _tree.TREE_LEAF:  # a split node
            if impurities[node_id] < impurity_threshold:
                # stop here, this will be a leaf
                removed_ns = prune_children(node_id, tree)
                # print("removed nodes below [%s]: %s" % (node_id, removed_ns))
            else:
                stop_on_min_impurity(left_id)
                stop_on_min_impurity(right_id)

    stop_on_min_impurity(0)


def build_models_and_get_pruning_info(tree, X_train_all, y_train_all,
                                      nodes_to_samples, leaf_model,
                                      node_models, global_std_dev,
                                      use_pruning, node_id=0):
    """


    :param tree: a tree that will be pruned on the way
    :param X:
    :param y:
    :param nodes_to_samples:
    :param leaf_model:
    :param node_models:
    :param global_std_dev:
    :param use_pruning:
    :param node_id:
    :return: a dictionary where the key is the feature index and the value is
        the number of samples where this feature is used
    """

    # Select the Error metric to compute model errors (used to compare node
    # with subtree for pruning)
    # TODO in Weka they use RMSE, but in papers they use MAE. could be a param
    # TODO Shall we go further and store the residuals, or a bunch of metrics?
    err_metric = root_mean_squared_error  # mean_absolute_error

    # Get the samples associated with this node
    samples_at_this_node = get_samples_at_node(node_id, nodes_to_samples)
    #print(samples_at_this_node)
    #print(set(samples_at_this_node).difference(set(y_train_all.index)))
    n_samples_at_this_node = samples_at_this_node.shape[0]
    y_true_this_node = y_train_all.iloc[samples_at_this_node]

    # Is this a leaf node or a split node ?
    left_node = tree.children_left[node_id]  # this way we do not have to query it again in the else.
    if left_node == _tree.TREE_LEAF:
        # --> Current node is a LEAF. See is_leaf(node_id, tree) if you have doubts <--

        # -- create a linear model for this node
        # leaves do not have the right to use any features since they have no
        # subtree: keep the constant prediction
        y_pred_this_node = ConstantLeafModel.predict_cstt(
            tree, node_id, y_true_this_node.shape[0]
        )
        err_this_node = err_metric(y_true_this_node, y_pred_this_node)

        # TODO when use_pruning = False, should we allow all features to be
        #   used instead of having to stick to the M5 rule of "only use a
        #   feature if the subtree includes a split with this feature" ?
        #   OR alternate proposal: should we transform the boolean use_pruning
        #   into a use_pruning_max_nb integer to say for example "only 2 level
        #   of pruning" ?

        # -- store the model information
        node_models[node_id] = ConstantLeafModel(err_this_node)

        # -- return an empty dict for features used
        return dict()  # np.array([], dtype=int)

    else:
        # --> Current node is a SPLIT <--
        right_node = tree.children_right[node_id]

        # (1) prune left and right subtree and get some information
        features_l = build_models_and_get_pruning_info(
            tree, X_train_all, y_train_all, nodes_to_samples, leaf_model,
            node_models, global_std_dev, use_pruning, node_id=left_node
        )
        features_r = build_models_and_get_pruning_info(
            tree, X_train_all, y_train_all, nodes_to_samples, leaf_model,
            node_models, global_std_dev, use_pruning, node_id=right_node
        )

        # (2) select only the samples that reach this node
        X_this_node = X_train_all[samples_at_this_node, :]

        # (3) Create a model for this node
        # -- fit a linear regression model taking into account only the
        # features used in the subtrees + the split one
        # TODO should we normalize=True (variance scaling) or use a whole
        #  pipeline here ? Not sure..
        # estimators = [('scale', StandardScaler()), ('clf', model_type())];
        # pipe = Pipeline(estimators)
        # skmodel_this_node = leaf_model_type(**leaf_model_params)
        skmodel_this_node = clone(leaf_model)

        # -- old - we used to only store the array of features
        # selected_features = np.union1d(features_l, features_r)
        # selected_features = np.union1d(selected_features, tree.feature[node_id])
        # -- new - we also gather the nb samples where this feature is used
        selected_features_dct = features_l
        for feature_id, n_samples_feat_used in features_r.items():
            if feature_id in selected_features_dct.keys():
                selected_features_dct[feature_id] += n_samples_feat_used
            else:
                selected_features_dct[feature_id] = n_samples_feat_used
        selected_features_dct[tree.feature[node_id]] = n_samples_at_this_node
        # -- use only the selected features, in the natural integer order
        selected_features = sorted(selected_features_dct.keys())

        X_train_this_node = X_this_node[:, selected_features]
        skmodel_this_node.fit(X_train_this_node, y_true_this_node)
        # -- predict and compute error
        y_pred_this_node = skmodel_this_node.predict(X_train_this_node)
        err_this_node = err_metric(y_true_this_node, y_pred_this_node)
        # -- create the object
        # TODO the paper suggest to perform recursive feature elimination in
        #  this model until adjusted_err_model is minimal, is it same in Weka ?
        model_this_node = LinRegLeafModel(selected_features, skmodel_this_node,
                                          err_this_node)

        # (4) compute adj error criterion = ERR * (n+v)/(n-v) for both models
        adjusted_err_model = compute_adjusted_error(err_this_node,
                                                    n_samples_at_this_node,
                                                    model_this_node.n_params)

        # (5) predict and compute adj error for the combination of child models
        # -- Note: this is recursive so the leaves may contain linear models
        # already
        y_pred_children = predict_from_leaves_no_smoothing(tree, node_models,
                                                           X_this_node)
        err_children = err_metric(y_true_this_node, y_pred_children)

        # TODO the Weka implem (below) differs from the paper that suggests a
        #  weigthed sum of adjusted errors. Why?
        # the total number of parameters if we do not prune, is the sum of
        # params in each branch PLUS 1 for the split
        n_params_splitmodel = (node_models[left_node].n_params
                               + node_models[right_node].n_params + 1)
        adjusted_err_children = compute_adjusted_error(
            err_children, n_samples_at_this_node, n_params_splitmodel
        )

        # (6) compare and either prune at this node or keep the subtrees
        std_dev_this_node = np.nanstd(y_true_this_node)
        # note: the first criterion is now already checked before that
        # function call, in `prune_on_min_impurity`
        if use_pruning and (
                std_dev_this_node < (global_std_dev * 0.05)
                or (adjusted_err_model <= adjusted_err_children)
                or (adjusted_err_model < (global_std_dev * 0.00001))  # so this means very good R² already
        ):
            # Choose model for this node rather than subtree model
            # -- prune from this node on
            removed_nodes = prune_children(node_id, tree)
            node_models[removed_nodes] = _tree.TREE_UNDEFINED

            # store the model information
            node_models[node_id] = model_this_node

            # update and return the features used
            selected_features_dct = {k: n_samples_at_this_node
                                     for k in selected_features_dct.keys()}
            return selected_features_dct

        else:
            # The subtrees are good or we do not want pruning: keep them.
            # This node will remain a split, and will only contain the digest
            # about the subtree
            node_models[node_id] = SplitNodeModel(
                n_params_splitmodel, err_children, selected_features,
                skmodel_this_node
            )

            # return the features used
            return selected_features_dct


def compute_adjusted_error(err, n_samples, n_parameters):
    """
    TODO why is this different from the term on the left of eq (4) in the
        paper ? (PRUNING_MULTIPLIER ?)
    TODO why 10 in the special case if n samples < n parameters ?

    :param n_samples: the number of samples
    :param n_parameters: number of parameters in the model
    :return:
    """
    #
    if n_samples <= n_parameters:
        # denominator is zero or negative: use a large factor so as to penalize
        # a lot this overly complex model.
        factor = 10.0  # Caution says Yong in his code
    else:
        factor = ((n_samples + PRUNING_MULTIPLIER * n_parameters)
                  / (n_samples - n_parameters))

    return err * factor


def predict_from_leaves_no_smoothing(tree, node_models, X):
    """
    Returns the prediction for all samples in X, based on using the appropriate
    model leaf.

    This function
     - uses tree.apply(X) to know in which tree leaf each sample in X goes
     - then for each of the leaves that are actually touched, uses
       node_models[leaf_id] to predict, for the X reaching that leaf

    This function assumes that node_models contains non-empty LinRegLeafModel
    entries for all leaf nodes that will be reached by the samples X.

    :param node_models:
    :param X: should contain all features !
    :return:
    """
    # **** This does the job, but we have one execution of model.predict() per
    # sample: probably not efficient
    # sample_ids_to_leaf_node_ids = tree.apply(X)
    # model_and_x = np.concatenate((node_models[leaf_node_ids].reshape(-1, 1), X), axis=1)
    # def pred(m_and_x):
    #     return m_and_x[0].model.predict(m_and_x[1:].reshape(1,-1))[0]
    # y_predicted = np.array(list(map(pred, model_and_x)))

    # **** This should be faster because the number of calls to predict() is
    # equal to the number of leaves touched
    sample_ids_to_leaf_node_ids = tree.apply(X)
    y_predicted = -np.ones(sample_ids_to_leaf_node_ids.shape, dtype=DOUBLE)

    # -- find the unique list of leaves touched
    leaf_node_ids, inverse_idx = np.unique(sample_ids_to_leaf_node_ids,
                                           return_inverse=True)

    # -- for each leaf perform the prediction for samples reaching that leaf
    for leaf_node_id in leaf_node_ids:
        # get the indices of the samples reaching that leaf
        sample_indices = np.nonzero(
            sample_ids_to_leaf_node_ids == leaf_node_id)[0]

        # predict
        node_model = node_models[leaf_node_id]
        if isinstance(node_model, LinRegLeafModel):
            y_predicted[sample_indices] = np.ravel(node_model.predict(
                X[sample_indices, :]))
        else:
            # isinstance(node_model, ConstantLeafModel)
            y_predicted[sample_indices] = tree.value[leaf_node_id]

    # **** Recursive strategy: not used anymore
    # left_node = tree.children_left[node_id]  # this way we do not have to query it again in the else.
    # if left_node == _tree.TREE_LEAF:
    #     # --> Current node is a LEAF. See is_leaf(node_id, tree) <--
    #     y_predicted = node_models[node_id].model.predict()
    # else:
    #     # --> Current node is a SPLIT <--
    #     right_node = tree.children_right[node_id]
    #
    #
    #     samples_at_this_node = get_samples_at_node(node_id, nodes_to_samples)
    #     y_true_this_node = y_train_all[samples_at_this_node]
    #     # As surprising as it may seem, in numpy [samples_at_this_node, selected_features] does something else.
    #     X_train_this_node = X_train_all[samples_at_this_node, :][:, selected_features]
    #
    #     X_left

    return y_predicted


def predict_from_leaves(m5p, X, smoothing=True, smoothing_constant=15):
    """
    Predicts using the M5P tree, without using the compiled sklearn
    `tree.apply` subroutine.

    The main purpose of this function is to apply smoothing to a M5P model tree
    where smoothing has not been pre-installed on the models. For examples to
    enable a model to be used both without and with smoothing for comparisons
    purposes, or for models whose leaves are not Linear Models and therefore
    for which no pre-installation method exist.

    Note: this method is slower than `predict_from_leaves_no_smoothing` when
    `smoothing=False`.

    :param m5p:
    :param X: input data for which the predictions should be made.
    :return:
    """
    # validate and converts dtype just in case this was directly called
    # e.g. in unit tests
    X = m5p._validate_X_predict(X, check_input=True)

    tree = m5p.tree_
    node_models = m5p.node_models
    nb_samples = X.shape[0]
    y_predicted = -np.ones((nb_samples, 1), dtype=DOUBLE)

    # sample_ids_to_leaf_node_ids = tree.apply(X)
    def smooth_predictions(ancestor_nodes, X_at_node, y_pred_at_node):
        # note: y_predicted_at_node can be a constant
        current_node_model_id = ancestor_nodes[-1]
        for i, parent_model_id in enumerate(reversed(ancestor_nodes[:-1])):
            # warning: this is the nb of TRAINING samples at this node
            node_nb_train_samples = tree.n_node_samples[current_node_model_id]
            parent_model = node_models[parent_model_id]
            parent_predictions = parent_model.predict(X_at_node)
            y_pred_at_node = ((node_nb_train_samples * y_pred_at_node
                               + smoothing_constant * parent_predictions)
                              / (node_nb_train_samples + smoothing_constant))
            current_node_model_id = parent_model_id

        return y_pred_at_node

    def apply_prediction(node_id, ids=None, ancestor_nodes=None):
        first_call = False
        if ids is None:
            ids = slice(None)
            first_call = True
        if smoothing:
            if ancestor_nodes is None:
                ancestor_nodes = [node_id]
            else:
                ancestor_nodes.append(node_id)

        left_id = tree.children_left[node_id]
        if left_id == _tree.TREE_LEAF:
            # ... and tree.children_right[node_id] == _tree.TREE_LEAF
            # LEAF node: predict
            # predict
            node_model = node_models[node_id]
            # assert (ids == (sample_ids_to_leaf_node_ids == node_id)).all()
            if isinstance(node_model, LinRegLeafModel):
                X_at_node = X[ids, :]
                predictions = node_model.predict(X_at_node)
            else:
                # isinstance(node_model, ConstantLeafModel)
                predictions = tree.value[node_id]
                if smoothing:
                    X_at_node = X[ids, :]

            # finally apply smoothing
            if smoothing:
                y_predicted[ids] = smooth_predictions(
                    ancestor_nodes, X_at_node, predictions)
            else:
                y_predicted[ids] = predictions
        else:
            right_id = tree.children_right[node_id]
            # non-leaf node: split samples and recurse
            left_group = np.zeros(nb_samples, dtype=bool)
            left_group[ids] = (X[ids, tree.feature[node_id]]
                               <= tree.threshold[node_id])
            right_group = ((~left_group)
                           if first_call
                           else (ids & (~left_group)))

            # important: copy ancestor_nodes BEFORE calling anything, otherwise
            # it will be modified
            apply_prediction(left_id, ids=left_group,
                             ancestor_nodes=(ancestor_nodes.copy()
                                             if ancestor_nodes is not None
                                             else None))
            apply_prediction(right_id, ids=right_group,
                             ancestor_nodes=ancestor_nodes)

    # recurse to fill all predictions
    apply_prediction(0)

    return y_predicted


def prune_children(node_id, tree):
    """
    Prunes the children of node_id in the given `tree`.

    Inspired by https://github.com/shenwanxiang/sklearn-post-prune-tree/blob/master/tree_prune.py#L122

    IMPORTANT this relies on the fact that `children_left` and `children_right`
    are modificable (and are the only things we need to modify to fix the
    tree). It seems to be the case as of now.

    :return: a list of removed nodes
    """

    def _prune_below(_node_id):
        left_child = tree.children_left[_node_id]
        right_child = tree.children_right[_node_id]
        if left_child == _tree.TREE_LEAF:
            # _node_id is a leaf: left_ & right_child say "leaf".Nothing to do
            return []
        else:
            # Make sure everything is pruned below: children should be leaves
            removed_l = _prune_below(left_child)
            removed_r = _prune_below(right_child)

            # -- first declare that they are not leaves anymore but they do not exist at all
            assert tree.children_left[left_child] == _tree.TREE_LEAF  # left_child was a leaf
            assert tree.children_right[left_child] == _tree.TREE_LEAF  # left_child was a leaf
            tree.children_left[left_child] = _tree.TREE_UNDEFINED
            tree.children_right[left_child] = _tree.TREE_UNDEFINED
            assert tree.children_left[right_child] == _tree.TREE_LEAF  # right_child was a leaf
            assert tree.children_right[right_child] == _tree.TREE_LEAF  # right_child was a leaf
            tree.children_left[right_child] = _tree.TREE_UNDEFINED
            tree.children_right[right_child] = _tree.TREE_UNDEFINED

            # -- then declare that current node is a leaf
            tree.children_left[_node_id] = _tree.TREE_LEAF
            tree.children_right[_node_id] = _tree.TREE_LEAF

            # return the list of removed nodes
            return removed_l + removed_r + [left_child, right_child]

    # true_node_count = tree.node_count - sum(tree.children_left == _tree.TREE_UNDEFINED)
    # do we need to change node_count here ? not sure, it may have side effects... and we clean all of this later so ok.
    # tree.node_count -= 2*len(nodes_to_remove)

    return _prune_below(node_id)


def is_leaf(node_id, tree):
    """
    Returns true if node with id `node_id` is a leaf in tree `tree`.
    Is is not actually used in this file because we always need the left child
    node id in our code.

    But it is kept here as an easy way to remember how it works.

    :param node_id:
    :param tree:
    :return:
    """
    if node_id == _tree.TREE_LEAF or node_id == _tree.TREE_UNDEFINED:
        raise ValueError("Invalid node_id %s" % node_id)

    return tree.children_left[node_id] == _tree.TREE_LEAF


def get_samples_at_node(node_id, nodes_to_samples):
    """
    Returns an array containing the ids of the samples for node `node_id`.
    This method requires the user to
     - first compute the decision path for the sample matrix X
     - then convert it to a csc

    >>> samples_to_nodes = estimator.decision_path(X)  # returns a Scipy compressed sparse row matrix (CSR)
    >>> nodes_to_samples = samples_to_nodes.tocsc()    # we need the column equivalent (CSC)
    >>> samples = get_samples_at_node(node_id, nodes_to_samples)

    :param node_id:
    :param nodes_to_samples_csc:
    :return:
    """
    return nodes_to_samples.indices[nodes_to_samples.indptr[node_id]:
                                    nodes_to_samples.indptr[node_id + 1]]


# def prune_nodes_in_tree(nodes_to_remove, tree):
#     """
#     Transforms all nodes in nodes_to_remove into leaf nodes, and returns the
#     resulting tree (work is done on a copy)
#
#     :param nodes_to_remove:
#     :param tree:
#     :return:
#     """
#     # Create a copy of the inner tree
#     # tree._copy is gone, but this does the same thing
#     out_tree = _tree.Tree(*tree.__reduce__()[1])
#     out_tree.__setstate__(tree.__getstate__().copy())
#
#     for node in nodes_to_remove:
#         # TO DO: Add a Tree method to remove a branch of a tree
#         out_tree.children_left[out_tree.children_left[node]] = _tree.TREE_UNDEFINED
#         out_tree.children_right[out_tree.children_left[node]] = _tree.TREE_UNDEFINED
#         out_tree.children_left[out_tree.children_right[node]] = _tree.TREE_UNDEFINED
#         out_tree.children_right[out_tree.children_right[node]] = _tree.TREE_UNDEFINED
#         out_tree.children_left[node] = _tree.TREE_LEAF
#         out_tree.children_right[node] = _tree.TREE_LEAF
#
#     # FIX ME: currently should not change node_count, after deletion
#     # this is not number of nodes in the tree
#     # out_tree.node_count -= 2*len(nodes_to_remove)
#
#     return out_tree


class M5Prime(M5Base, RegressorMixin):
    """An M5' (M five prime) model tree regressor.

    The original M5 algorithm was invented by R. Quinlan. Y. Wang made
    improvements and named the resulting algorithm M5 Prime.

    This implementation was inspired by Weka (https://github.com/bnjmn/weka)
    M5Prime class, from Mark Hall

    Read more in the :ref:`User Guide <tree>`.

    See also
    --------
    M5Base

    References
    ----------

    .. [1] Ross J. Quinlan, "Learning with Continuous Classes", 5th Australian
           Joint Conference on Artificial Intelligence, pp343-348, 1992.
    .. [2] Y. Wang and I. H. Witten, "Induction of model trees for predicting
           continuous classes", Poster papers of the 9th European Conference
           on Machine Learning, 1997.

    """
    # def __init__(self, save_instances: bool=False):
    #     super(M5Prime, self).__init__(generate_rule=False,
    #                                   save_instances=save_instances)


def _empty_contig_ar(shape, dtype):
    return np.ascontiguousarray(np.empty(shape, dtype=dtype))

from numbers import Integral
from warnings import warn

import numpy as np
import six

from sklearn.tree._export import SENTINEL
from sklearn.tree._criterion import FriedmanMSE
#from sklearn import _tree


def export_text_m5(decision_tree, out_file=SENTINEL, max_depth=None,
                   feature_names=None, class_names=None, label='all',
                   target_name=None,
                   # filled=False, leaves_parallel=False,
                   impurity=True,
                   node_ids=False, proportion=False,
                   # rounded=False, rotate=False,
                   special_characters=False, precision=3, **kwargs):
    """Export a decision tree in TXT format.

    Note: this should be merged with ._export.export_text

    Inspired by WEKA and by
    >>> from sklearn.tree import export_graphviz

    This function generates a human-readable, text representation of the
    decision tree, which is then written into `out_file`.

    The sample counts that are shown are weighted with any sample_weights that
    might be present.

    Read more in the :ref:`User Guide <tree>`.

    Parameters
    ----------
    decision_tree : decision tree classifier
        The decision tree to be exported to text.

    out_file : file object or string, optional (default='tree.dot')
        Handle or name of the output file. If ``None``, the result is
        returned as a string. This will the default from version 0.20.

    max_depth : int, optional (default=None)
        The maximum depth of the representation. If None, the tree is fully
        generated.

    feature_names : list of strings, optional (default=None)
        Names of each of the features.

    class_names : list of strings, bool or None, optional (default=None)
        Names of each of the target classes in ascending numerical order.
        Only relevant for classification and not supported for multi-output.
        If ``True``, shows a symbolic representation of the class name.

    label : {'all', 'root', 'none'}, optional (default='all')
        Whether to show informative labels for impurity, etc.
        Options include 'all' to show at every node, 'root' to show only at
        the top root node, or 'none' to not show at any node.

    target_name : optional string with the target name. If not provided, the
        target will not be displayed in the equations

    impurity : bool, optional (default=True)
        When set to ``True``, show the impurity at each node.

    node_ids : bool, optional (default=False)
        When set to ``True``, show the ID number on each node.

    proportion : bool, optional (default=False)
        When set to ``True``, change the display of 'values' and/or 'samples'
        to be proportions and percentages respectively.

    special_characters : bool, optional (default=False)
        When set to ``False``, ignore special characters for PostScript
        compatibility.

    precision : int, optional (default=3)
        Number of digits of precision for floating point in the values of
        impurity, threshold and value attributes of each node.

    kwargs : other keyword arguments for the linear model printer

    Returns
    -------
    dot_data : string
        String representation of the input tree in GraphViz dot format.
        Only returned if ``out_file`` is None.

        .. versionadded:: 0.18

    Examples
    --------
    >>> from sklearn.datasets import load_iris
    >>> from sklearn import tree

    >>> clf = tree.DecisionTreeClassifier()
    >>> iris = load_iris()

    >>> clf = clf.fit(iris.data, iris.target)
    >>> tree_to_text(clf, out_file='tree.txt')                # doctest: +SKIP

    """

    models = []

    def add_model(node_model):
        models.append(node_model)
        return len(models)

    def node_to_str(tree, node_id, criterion, node_models=None):
        """ Generates the node content string """

        # Should labels be shown?
        labels = (label == 'root' and node_id == 0) or label == 'all'

        # PostScript compatibility for special characters
        if special_characters:
            characters = ['&#35;', '<SUB>', '</SUB>', '&le;', '<br/>', '>']
            node_string = '<'
        else:
            characters = ['#', '[', ']', '<=', '\\n', '']
            node_string = ''

        # -- If this node not a leaf,Write the split decision criteria (x <= y)
        leaf = is_leaf(node_id, tree)
        if not leaf:
            if feature_names is not None:
                feature = feature_names[tree.feature[node_id]]
            else:
                feature = "X%s%s%s" % (characters[1],
                                       tree.feature[node_id],  # feature id for the split
                                       characters[2])
            node_string += '%s %s %s' % (feature,
                                         characters[3],  # <=
                                         round(tree.threshold[node_id],   # threshold for the split
                                               precision))
        else:
            node_string += 'LEAF'

        # Node details - start bracket [
        node_string += ' %s' % characters[1]

        # -- Write impurity
        if impurity:
            if isinstance(criterion, FriedmanMSE):
                criterion = "friedman_mse"
            elif not isinstance(criterion, str):
                criterion = "impurity"
            if labels:
                node_string += '%s=' % criterion
            node_string += str(round(tree.impurity[node_id], precision)) + ', '

        # -- Write node sample count
        if labels:
            node_string += 'samples='
        if proportion:
            percent = (100. * tree.n_node_samples[node_id] /
                       float(tree.n_node_samples[0]))
            node_string += str(round(percent, 1)) + '%'
        else:
            node_string += str(tree.n_node_samples[node_id])

        # Node details - end bracket ]
        node_string += '%s' % characters[2]

        # -- Write node class distribution / regression value
        if tree.n_outputs == 1:
            value = tree.value[node_id][0, :]
        else:
            value = tree.value[node_id]

        if proportion and tree.n_classes[0] != 1:
            # For classification this will show the proportion of samples
            value = value / tree.weighted_n_node_samples[node_id]
        if tree.n_classes[0] == 1:
            # Regression
            value_text = np.around(value, precision)
        elif proportion:
            # Classification
            value_text = np.around(value, precision)
        elif np.all(np.equal(np.mod(value, 1), 0)):
            # Classification without floating-point weights
            value_text = value.astype(int)
        else:
            # Classification with floating-point weights
            value_text = np.around(value, precision)

        # Strip whitespace
        value_text = str(value_text.astype('S32')).replace("b'", "'")
        value_text = value_text.replace("' '", ", ").replace("'", "")
        if tree.n_classes[0] == 1 and tree.n_outputs == 1:
            value_text = value_text.replace("[", "").replace("]", "")
        value_text = value_text.replace("\n ", characters[4])

        if node_models is None:
            node_string += ' : '
            if labels:
                node_string += 'value='
        else:
            nodemodel = node_models[node_id]
            model_err_val = np.around(nodemodel.error, precision)
            if leaf:
                if isinstance(nodemodel, ConstantLeafModel):
                    # the model does not contain the value. rely on the value_text computed from tree
                    value_text = " : %s (err=%s, params=%s)" % (value_text, model_err_val, nodemodel.n_params)
                else:
                    # put the model in the stack, we'll write it later
                    model_id = add_model(nodemodel)
                    value_text = " : LM%s (err=%s, params=%s)" % (model_id, model_err_val, nodemodel.n_params)
            else:
                # replace the value text with error at this node and number of parameters
                value_text = " (err=%s, params=%s)" % (model_err_val, nodemodel.n_params)

        node_string += value_text

        # Write node majority class
        if (class_names is not None and
                tree.n_classes[0] != 1 and
                tree.n_outputs == 1):
            # Only done for single-output classification trees
            node_string += ', '
            if labels:
                node_string += 'class='
            if class_names is not True:
                class_name = class_names[np.argmax(value)]
            else:
                class_name = "y%s%s%s" % (characters[1],
                                          np.argmax(value),
                                          characters[2])
            node_string += class_name

        return node_string + characters[5]

    def recurse(tree, node_id, criterion, parent=None, depth=0, node_models=None):
        if node_id == _tree.TREE_LEAF:
            raise ValueError("Invalid node_id %s" % _tree.TREE_LEAF)

        # Add node with description
        if max_depth is None or depth <= max_depth:
            indent_str = ("|   " * depth)
            if node_ids:
                out_file.write('%d| %s%s\n' % (node_id, indent_str, node_to_str(tree, node_id, criterion,
                                                                                node_models=node_models)))
            else:
                out_file.write('%s%s\n' % (indent_str, node_to_str(tree, node_id, criterion, node_models=node_models)))

            # Recurse on Children if needed
            left_child = tree.children_left[node_id]
            right_child = tree.children_right[node_id]

            if left_child != _tree.TREE_LEAF:
                # that means that node_id is not a leaf (see is_leaf() below.): recurse on children
                recurse(tree, left_child, criterion=criterion, parent=node_id, depth=depth + 1,
                        node_models=node_models)
                recurse(tree, right_child, criterion=criterion, parent=node_id, depth=depth + 1,
                        node_models=node_models)

        else:
            ranks['leaves'].append(str(node_id))
            out_file.write('%d| (...)\n')

    def write_models(models):
        for i, model in enumerate(models):
            out_file.write("LM%s: %s\n" % (i + 1, model.to_text(feature_names=feature_names, precision=precision,
                                                                target_name=target_name, **kwargs)))

    # Main
    check_is_fitted(decision_tree, 'tree_')
    own_file = False
    return_string = False
    try:
        if out_file == SENTINEL:
            warn("out_file can be set to None starting from 0.18. This will be"
                 "the default in 0.20.", DeprecationWarning)
            out_file = "tree.txt"

        if isinstance(out_file, str):
            out_file = open(out_file, "w", encoding="utf-8")
            own_file = True

        if out_file is None:
            return_string = True
            out_file = six.StringIO()

        if isinstance(precision, Integral):
            if precision < 0:
                raise ValueError("'precision' should be greater or equal to 0."
                                 " Got {} instead.".format(precision))
        else:
            raise ValueError("'precision' should be an integer. Got {}"
                             " instead.".format(type(precision)))

        # Check length of feature_names before getting into the tree node
        # Raise error if length of feature_names does not match
        # n_features_ in the decision_tree
        if feature_names is not None:
            if len(feature_names) != decision_tree.n_features_:
                raise ValueError("Length of feature_names, %d "
                                 "does not match number of features, %d"
                                 % (len(feature_names),
                                    decision_tree.n_features_))

        # The depth of each node for plotting with 'leaf' option TODO probably remove
        ranks = {'leaves': []}

        # Tree title
        if isinstance(decision_tree, M5Base):
            if hasattr(decision_tree, 'installed_smoothing_constant'):
                details = "pre-smoothed with constant %s" % decision_tree.installed_smoothing_constant
            else:
                if decision_tree.use_smoothing == 'installed':
                    details = "under construction - not pre-smoothed yet"
                else:
                    details = "unsmoothed - but this can be done at prediction time"

            # add more info or M5P
            out_file.write('%s (%s):\n' % (type(decision_tree).__name__, details))
        else:
            # generic title
            out_file.write('%s :\n' % type(decision_tree).__name__)

        # some space for readability
        out_file.write('\n')

        # Now recurse the tree and add node & edge attributes
        if isinstance(decision_tree, _tree.Tree):
            recurse(decision_tree, 0, criterion="impurity")
        elif isinstance(decision_tree, M5Base) and hasattr(decision_tree, 'node_models'):
            recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion, node_models=decision_tree.node_models)

            # extra step: write all models
            out_file.write("\n")
            write_models(models)
        else:
            recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)

        # Return the text if needed
        if return_string:
            return out_file.getvalue()

    finally:
        if own_file:
            out_file.close()

